{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "491d6886-ffeb-42d5-a873-f5db7b231361",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8acb90cc-bd56-4e90-8daa-d4b6a23cad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d8d7c07-e7a2-4bb2-b7f8-00f6d507c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer.\n",
    "class Layer:\n",
    "    def __init__(self, layer_neurons, activation = \"linear\"):\n",
    "        \n",
    "        self.layer_neurons = layer_neurons\n",
    "        self.act = activation\n",
    "        self.activation = getattr(self, \"_\" + activation)\n",
    "        self.activation_derivative = getattr(self, \"_\" + activation + \"_derivative\")\n",
    "        self.preActivation = None\n",
    "        self.posActivation = None\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Activation layer functions.\n",
    "    \"\"\" \n",
    "    def _linear(self, x):\n",
    "        return x\n",
    "    \n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def _tanh(self, x):\n",
    "        return 2 * self._sigmoid(2*x) - 1\n",
    "    \n",
    "    def _relu(self, x):\n",
    "        return np.maximum(x, np.zeros(len(x)))\n",
    "    \n",
    "    def _hardTanh(self, x):\n",
    "        return np.maximum(np.minimum(x, np.zeros(len(x)) + 1), np.zeros(len(x)))\n",
    "\n",
    "    \"\"\"\n",
    "    Derivative activation functions.\n",
    "    \"\"\"\n",
    "    def _linear_derivative(self, x):\n",
    "        return np.zeros(len(x)) + 1\n",
    "    \n",
    "    def _sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def _tanh_derivative(self, x):\n",
    "        return (1 - x**2)\n",
    "    \n",
    "    def _relu_derivative(self, x):\n",
    "        return np.maximum(x, np.zeros(len(x)))\n",
    "\n",
    "# Neural Network.\n",
    "class NeuralNetwork:\n",
    "        \n",
    "    \"\"\"\n",
    "    Initialize NN.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_neurons):\n",
    "        \n",
    "        # Input layer.\n",
    "        input_layer = Layer(input_neurons)\n",
    "        \n",
    "        # Layers.\n",
    "        self.layers = list()\n",
    "        self.layers.append(input_layer)\n",
    "        \n",
    "        # Weights.\n",
    "        self.weights = list()\n",
    "        \n",
    "        # Loss function.\n",
    "        self.loss = None\n",
    "        \n",
    "        self.learning_rate = 1\n",
    "    \n",
    "    \"\"\"\n",
    "    Add a new layer to the NN.\n",
    "    \"\"\"\n",
    "    def add_layer(self, layer_neurons, activation):\n",
    "        layer = Layer(layer_neurons, activation = activation)\n",
    "        self.layers.append(layer)\n",
    "        \n",
    "    \"\"\"\n",
    "    Forward Phase.\n",
    "    \"\"\"\n",
    "    def predict(self, vector_x, print_forward = False):\n",
    "        \n",
    "        # Input layer.\n",
    "        self.layers[0].preActivation = vector_x \n",
    "        self.layers[0].posActivation = vector_x\n",
    "        \n",
    "        if print_forward: \n",
    "            print(\"Input Layer: \", 0)\n",
    "            print(\"Pre Activation: \", self.layers[0].preActivation)\n",
    "            print(\"Pos Activation: \", self.layers[0].posActivation)\n",
    "        \n",
    "        # Hidden Layers.\n",
    "        for i in range(1, len(self.layers)):\n",
    "                \n",
    "            # Compute preactivation values of current layer.\n",
    "            self.layers[i].preActivation = np.dot(vector_x, self.weights[i - 1])\n",
    "            \n",
    "            # Compute posactivation values of current layer.\n",
    "            self.layers[i].posActivation = self.layers[i].activation(self.layers[i].preActivation)\n",
    "            \n",
    "            # Update vector x.\n",
    "            vector_x = self.layers[i].posActivation\n",
    "            \n",
    "            # Print values.\n",
    "            if print_forward: \n",
    "                print(\"\")\n",
    "                print(\"Hidden Layer: \", i)\n",
    "                print(\"Activation: \", self.layers[i].activation.__name__.replace(\"_\", \"\"))\n",
    "                print(\"Pre Activation: \", self.layers[i].preActivation)\n",
    "                print(\"Pos Activation: \", self.layers[i].posActivation)\n",
    "            \n",
    "        # Return result.\n",
    "        return vector_x\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Set loss function & weights.\n",
    "    \"\"\"\n",
    "    def initialize_nn(self, loss = \"MSE\", learning_rate = 1):\n",
    "        self.loss = getattr(self, \"_\" + loss)\n",
    "        self.loss_derivative = getattr(self, \"_\" + loss + \"_derivative\")\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        for i in range(len(self.layers) - 1): \n",
    "            matrix_weights = np.random.rand(self.layers[i].layer_neurons, self.layers[i+1].layer_neurons)\n",
    "            self.weights.append(matrix_weights)\n",
    "        \n",
    "        num_params = 0\n",
    "        for matrix in self.weights:\n",
    "            n, m = matrix.shape\n",
    "            num_params += n*m\n",
    "        print(\"Number of parameters: \", num_params)        \n",
    "\n",
    "    \"\"\"\n",
    "    Fit the model.\n",
    "    \"\"\"\n",
    "    def fit(self, X, Y, iterations = 1):\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            vector_x = X[0]\n",
    "            y_true = Y[0]\n",
    "            \n",
    "            for j in range(iterations):\n",
    "                \n",
    "                y_predicted = self.predict(vector_x, print_forward = False)\n",
    "                error = self.loss(y_true, y_predicted)\n",
    "                \n",
    "                # Compute deltas(hr, o): d(L) / d(ah)_r\n",
    "                gradients = self.back_propagation(error)\n",
    "                \n",
    "                # Compute d(L)/d(W_{hr-1, hr})\n",
    "                weights_gradients = list()                    \n",
    "                for i in range(0, len(gradients)):\n",
    "                    h_rminus1_posActivationValues = self.layers[len(self.layers) - (i+2)].preActivation\n",
    "                    \n",
    "                    rows = list()\n",
    "                    for value in h_rminus1_posActivationValues: \n",
    "                        row = gradients[i] * value\n",
    "                        rows.append(row)\n",
    "                    matrix_gradient = np.array(rows)\n",
    "                    weights_gradients.append(matrix_gradient)\n",
    "                                        \n",
    "                # Gradient Descent!\n",
    "                for i in range(len(self.weights)): \n",
    "                    self.weights[i] = self.weights[i] - self.learning_rate * weights_gradients[-(i + 1)]\n",
    " \n",
    "\n",
    "    \"\"\"\n",
    "    Backpropagation algorithm.\n",
    "    \"\"\"\n",
    "    def back_propagation(self, error):\n",
    "        \n",
    "        gradients = list()\n",
    "        \n",
    "        # Output layer.\n",
    "        last_layer = self.layers[-1]\n",
    "        gradient = self.loss_derivative(error)\n",
    "        gradient = gradient * last_layer.activation_derivative(last_layer.preActivation)\n",
    "        gradients.append(gradient)\n",
    "\n",
    "        # Hidden Layers.\n",
    "        for i in range(2, len(self.layers)):\n",
    "            hidden_layer = self.layers[-i]\n",
    "            gradient = np.dot(gradient, np.transpose(self.weights[-i + 1]))\n",
    "            gradient = np.multiply(gradient, hidden_layer.activation_derivative(hidden_layer.preActivation))\n",
    "            gradients.append(gradient)\n",
    "            \n",
    "        return gradients\n",
    "            \n",
    "    \"\"\" \n",
    "    Loss functions. \n",
    "    \"\"\"\n",
    "    def _MSE(self, y_true, y_predicted):\n",
    "        return (y_true - y_predicted)**2\n",
    "    \n",
    "    def _MSE_derivative(self, error):\n",
    "        return 2 * (error) * -1\n",
    "    \n",
    "    \"\"\" \n",
    "    Drawing Function \n",
    "    \"\"\"\n",
    "    def draw_nn(self):\n",
    "        G = nx.Graph() # creates a graph\n",
    "        for i in range(len(self.layers)):\n",
    "            for j in range(self.layers[i].layer_neurons):\n",
    "                G.add_node(\"Capa \"+str(i)+\": Neu \"+str(j)+\"\\n\"+self.layers[i].act)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            for j in range(self.layers[i].layer_neurons):\n",
    "                if i+1 < len(self.layers):\n",
    "                    for k in range(self.layers[i+1].layer_neurons):\n",
    "                        G.add_edge(\"Capa \"+str(i)+\": Neu \"+str(j)+\"\\n\"+self.layers[i].act, \"Capa \"+str(i+1)+\": Neu \"+str(k)+\"\\n\"+self.layers[i+1].act)\n",
    "        A = nx.nx_agraph.to_agraph(G)\n",
    "        A.layout('dot')\n",
    "        A.draw('grafo.png') # saves as png\n",
    "        # graphviz.Source(A.to_string()) # shows in jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992f8631-ce0b-4725-a590-05a38e917568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  39\n"
     ]
    }
   ],
   "source": [
    "nn = NeuralNetwork(input_neurons = 6) \n",
    "nn.add_layer(layer_neurons = 4, activation = \"relu\")\n",
    "nn.add_layer(layer_neurons = 3, activation = \"sigmoid\")\n",
    "nn.add_layer(layer_neurons = 1, activation = \"tanh\")\n",
    "nn.initialize_nn(loss = \"MSE\", learning_rate = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9ef7fd-8c00-4aa6-9d93-8de546ddeb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.\n",
    "X = [[0.98, 0.89, 1.33, -1.5, 0.322, -0.237]]\n",
    "Y = [0.83]\n",
    "# Fit the NN.\n",
    "nn.fit(X, Y, iterations = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "878fdc02-2605-44fb-b4dd-a9efa090b751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Layer:  0\n",
      "Pre Activation:  [[0.98, 0.89, 1.33, -1.5, 0.322, -0.237]]\n",
      "Pos Activation:  [[0.98, 0.89, 1.33, -1.5, 0.322, -0.237]]\n",
      "\n",
      "Hidden Layer:  1\n",
      "Activation:  relu\n",
      "Pre Activation:  [[2.87014136 2.28821834 3.76131261 0.2851881 ]]\n",
      "Pos Activation:  [[2.87014136 2.28821834 3.76131261 0.2851881 ]]\n",
      "\n",
      "Hidden Layer:  2\n",
      "Activation:  sigmoid\n",
      "Pre Activation:  [[8.9779337  5.99943103 5.01944212]]\n",
      "Pos Activation:  [[0.99987385 0.99752597 0.99343517]]\n",
      "\n",
      "Hidden Layer:  3\n",
      "Activation:  tanh\n",
      "Pre Activation:  [[1.18857969]]\n",
      "Pos Activation:  [[0.83013786]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.83013786]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict values.\n",
    "nn.predict(X, print_forward = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "245e0c25-5d22-4d51-9810-8332d1041e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.draw_nn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af586f73-d1f0-42ee-bd8f-24b71e1476a7",
   "metadata": {},
   "source": [
    "![title](grafo.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
