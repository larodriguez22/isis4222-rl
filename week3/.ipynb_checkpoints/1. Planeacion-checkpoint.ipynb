{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment():\n",
    "    \n",
    "    def __init__(self, fin_x, fin_y, route_img):\n",
    "        self.fin_x = fin_x\n",
    "        self.fin_y = fin_y\n",
    "        \n",
    "        \n",
    "        # RECOMPENSA\n",
    "        self.rw = -1 # Living (Movement) Penalty\n",
    "        \n",
    "        # CARGA IMAGEN\n",
    "        im = Image.open(route_img, 'r')\n",
    "        \n",
    "        # DIMENSIONES\n",
    "        self.columns = im.size[0]\n",
    "        self.rows = im.size[1]\n",
    "        \n",
    "        # INICIALIZACION\n",
    "        pix_val = list(im.getdata())\n",
    "        self.new_pix = [x[0] for x in pix_val]\n",
    "        self.walls_and_paths = np.ones([self.rows,self.columns])\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.columns):\n",
    "                if self.new_pix[i*self.columns+j] == 0:\n",
    "                    self.walls_and_paths[i,j] = 0\n",
    "        self.rewards = self.rw*np.ones([self.rows,self.columns])\n",
    "        \n",
    "        # Cambiar el valor\n",
    "        self.value_state_table = np.zeros([self.rows,self.columns])\n",
    "        self.value_state_table[fin_x][fin_y]=1000\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def dibujo_ruta (array_pos):\n",
    "        matrixMap = np.ones([self.rows,self.columns])\n",
    "        RGBMap = []\n",
    "\n",
    "        for i in range(self.rows):\n",
    "            for j in range(self.columns):\n",
    "                matrixMap[i,j] = self.walls_and_paths[i,j]\n",
    "\n",
    "        for i in range(len(array_pos[:,0])):\n",
    "            matrixMap[array_pos[i,1],array_pos[i,0]] = 2\n",
    "\n",
    "        for i in range(self.rows):\n",
    "            file = []\n",
    "            for j in range(self.columns):\n",
    "                if matrixMap[i,j] == 1:\n",
    "                    RGB = [255,255,255,255]                     #RGBA de color blanco\n",
    "                elif matrixMap[i,j] == 0:\n",
    "                    RGB = [0,0,0,255]                           #RGBA de color negro\n",
    "                else:\n",
    "                    RGB = [255,0,0,255]                         #RGBA de color rojo\n",
    "                file.append(RGB)\n",
    "            RGBMap.append(file)\n",
    "\n",
    "        RGBMap = np.array(RGBMap).astype(np.uint8)    #El m√≥dulo Image necesita los array definidos de esta forma.\n",
    "        routeMap = Image.fromarray(RGBMap, \"RGBA\")\n",
    "        routeMap.save(\"MapaRuta.png\")\n",
    "        return routeMap\n",
    "    \n",
    "    \n",
    "    def getStateValue(self, position):\n",
    "        return self.value_state_table[position[0]][position[1]]\n",
    "    \n",
    "    def getValue_state_table(self):\n",
    "        return self.value_state_table\n",
    "    \n",
    "    def reset(self, fin_x, fin_y):\n",
    "        self.value_state_table = np.zeros([self.rows,self.columns])\n",
    "        self.value_state_table[fin_x][fin_y]=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class valueBasedAgent():\n",
    "    \n",
    "    def __init__(self, environment, policy, discount_factor):\n",
    "        self.pos = [0,0]\n",
    "        self.total_reward = 0\n",
    "        self.environment = environment\n",
    "        self.discount_factor = discount_factor\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        \n",
    "        # Start with a random policy. 0.25 chance of moving to any direction.\n",
    "        self.policy = policy   \n",
    "            \n",
    "    def forwardState(self, pos, action):\n",
    "        \n",
    "        # New position array.\n",
    "        tam_y = len(environment.value_state_table[0])-1\n",
    "        tam_x = len(environment.value_state_table)-1\n",
    "        new_position = pos\n",
    "        \n",
    "        # Compute new position based on action taken.\n",
    "        if(action == \"up\" and pos[1] < tam_y ):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] + 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] + 1]\n",
    "\n",
    "        elif(action == \"down\" and pos[1] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0]][pos[1] - 1]) == 1:\n",
    "                new_position = [pos[0], pos[1] - 1]\n",
    "                \n",
    "        elif(action == \"left\" and pos[0] > 0):\n",
    "            if(self.environment.walls_and_paths[pos[0] - 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] - 1, pos[1]]\n",
    "\n",
    "        elif(action == \"right\" and pos[0] < tam_x):\n",
    "            if(self.environment.walls_and_paths[pos[0] + 1][pos[1]]) == 1:\n",
    "                new_position = [pos[0] + 1, pos[1]]\n",
    "        return new_position\n",
    "        \n",
    "        \n",
    "    def valueFunction(self):\n",
    "            \n",
    "        # Initialize variable.\n",
    "        new_state_value = 0\n",
    "    \n",
    "        # Random movement! - Cuando aun no se ha inicializado \n",
    "        if self.policy[self.pos[0]][self.pos[1]] == \"r\":\n",
    "            for action in self.actions:        \n",
    "                forward_state = self.forwardState(self.pos, action)\n",
    "                \n",
    "                # Simplified version of Q-value. BELLMANS EQUATION\n",
    "                q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "                new_state_value += q_value * 0.25 # Probabilidad de 0.25 para cada una de las acciones\n",
    "            return new_state_value\n",
    "        \n",
    "        # Not random movement!\n",
    "        else: \n",
    "            action = self.policy[self.pos[0]][self.pos[1]]\n",
    "            forward_state = self.forwardState(self.pos, action)\n",
    "            \n",
    "            # Simplified version of Q-value.\n",
    "            q_value = (self.environment.rewards[forward_state[0]][forward_state[1]] \n",
    "                                    + self.discount_factor * self.environment.value_state_table[forward_state[0]][forward_state[1]])\n",
    "            new_state_value += q_value # Probabilidad de 1\n",
    "            return new_state_value\n",
    "        \n",
    "    def getPosition(self):\n",
    "        return self.pos\n",
    "    \n",
    "    def getReward(self):\n",
    "        return self.total_reward\n",
    "    \n",
    "    def setPosition(self, x, y):\n",
    "        self.pos = [x, y]\n",
    "        \n",
    "    def updateValueStateTable(self):\n",
    "        new_state_value = self.valueFunction()\n",
    "        self.environment.value_state_table[self.pos[0]][self.pos[1]] = new_state_value\n",
    "        \n",
    "    def selectBestAction(self):\n",
    "        \n",
    "        # Compute new possible states.\n",
    "        go_up = self.forwardState(self.pos, \"up\")\n",
    "        go_down = self.forwardState(self.pos, \"down\")\n",
    "        go_left = self.forwardState(self.pos, \"left\")\n",
    "        go_right = self.forwardState(self.pos, \"right\")\n",
    "        \n",
    "        # Q values (simplified version).\n",
    "        up_value = (self.environment.rewards[go_up[0]][go_up[1]] + \n",
    "                    self.discount_factor * self.environment.value_state_table[go_up[0]][go_up[1]])\n",
    "        down_value = (self.environment.rewards[go_down[0]][go_down[1]] + \n",
    "                      self.discount_factor * self.environment.value_state_table[go_down[0]][go_down[1]])\n",
    "        left_value = (self.environment.rewards[go_left[0]][go_left[1]] + \n",
    "                        self.discount_factor * self.environment.value_state_table[go_left[0]][go_left[1]])\n",
    "        right_value = (self.environment.rewards[go_right[0]][go_right[1]] + \n",
    "                       self.discount_factor * self.environment.value_state_table[go_right[0]][go_right[1]])\n",
    "        \n",
    "        # Array of Q-values.\n",
    "        values = [up_value, down_value, left_value, right_value]\n",
    "        \n",
    "        best_action = self.actions[values.index(max(values))] \n",
    "        return best_action       \n",
    "            \n",
    "    def move(self):\n",
    "    \n",
    "        # Select action according to policy.\n",
    "        action = self.policy[self.pos[0]][self.pos[1]]\n",
    "\n",
    "        # Move to new position according to action taken.\n",
    "        self.pos = self.forwardState(self.pos, action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Politica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyEvaluation():\n",
    "    \n",
    "    def __init__(self,x_fin,y_fin, environment, agent, iterations = 3):\n",
    "        \n",
    "        self.x_fin = x_fin\n",
    "        self.y_fin = y_fin\n",
    "        self.environment = environment       \n",
    "        self.agent = agent\n",
    "        self.iterations = iterations\n",
    "    \n",
    "    def evaluate(self, plot_grid = True):\n",
    "        self.DP_policy_evaluation(self.iterations, plot_grid)\n",
    "        \n",
    "    def DP_policy_evaluation(self, iterations, plot_grid):\n",
    "        \n",
    "        for k in range(0, iterations):\n",
    "            print(\"Iteracion #\",k)\n",
    "            for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "\n",
    "                    if self.environment.walls_and_paths[i][j] == 1 and self.canChangeStateValue(i, j):\n",
    "\n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        self.agent.updateValueStateTable()\n",
    "                            \n",
    "    \n",
    "\n",
    "    def canChangeStateValue(self, x, y):\n",
    "        # Posicion que no se puede modificar\n",
    "        cant_modify = bool((x == self.x_fin and y == self.y_fin)) # or (x == 4 and y == 4))\n",
    "        \n",
    "        grid = self.environment.walls_and_paths\n",
    "        coords = list()\n",
    "        \n",
    "        # Get walls.\n",
    "        for i in range(0, len(grid)):\n",
    "            for j in range(0, len(grid[0])):\n",
    "                if grid[i][j] == 0:\n",
    "                    coords.append([i, j])\n",
    "        for c in coords: \n",
    "            if c == [x, y]:\n",
    "                cant_modify = True\n",
    "                break\n",
    "                \n",
    "        return not cant_modify\n",
    "    \n",
    "    def updatePolicy(self):\n",
    "        \n",
    "         for i in range(0, len(self.environment.value_state_table)):\n",
    "                for j in range(0, len(self.environment.value_state_table[0])):\n",
    "                    if self.environment.walls_and_paths[i][j] == 1:\n",
    "                        \n",
    "                        # Set agent position.\n",
    "                        self.agent.setPosition(i, j)\n",
    "                        best_action = self.agent.selectBestAction()\n",
    "                        self.agent.policy[i][j] = best_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game():\n",
    "    \n",
    "    def __init__(self,x_fin,y_fin, environment, agent):\n",
    "        \n",
    "        self.environment = environment       \n",
    "        self.agent = agent             \n",
    "        print(\"GridWorld Initialize!\")\n",
    "                \n",
    "    def update(self, secs):\n",
    "        \n",
    "        route = []\n",
    "        pos = self.agent.getPosition()\n",
    "        while not ((self.agent.pos[0] == x_fin and self.agent.pos[1] == y_fin)):\n",
    "            \n",
    "            self.agent.move()\n",
    "            pos = self.agent.getPosition()\n",
    "            \n",
    "            route.append(pos)\n",
    "        \n",
    "        print(\"array_pos\", route)\n",
    "        return route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 80)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import rospy\n",
    "from std_msgs.msg import String\n",
    "\n",
    "# Initialize environment and agent.\n",
    "x_fin = 0\n",
    "y_fin = 0\n",
    "x_ini = 0\n",
    "y_ini = 0\n",
    "x_len, y_len = 0,0\n",
    "discount_factor = 1\n",
    "ruta_imagen=\"imagen1.png\"\n",
    "\n",
    "def subscriber_ROS():\n",
    "    rospy.init_node('node', anonymous=True)\n",
    "    rospy.Subscriber(\"algo\")\n",
    "\n",
    "def publisher_ROS(array_pos):\n",
    "    rospy.init_node('node', anonymous=True)\n",
    "    rospy.Publisher(\"algo\", queue_size=0)\n",
    "\n",
    "# Generate the random policy.\n",
    "def run_policy(size):\n",
    "    policy = list()\n",
    "    for i in range(0, size[0]):\n",
    "        column = list()\n",
    "        for j in range(0, size[1]):\n",
    "            column.append(\"r\")\n",
    "        policy.append(column)\n",
    "    return policy\n",
    "\n",
    "def correct_policy(policy):\n",
    "    politica = list()\n",
    "    for i in range(len(policy)):\n",
    "        row = list()\n",
    "        for j in range(len(policy[0])):\n",
    "            if policy[i,j] == 'u':\n",
    "                row.append(\"up\")\n",
    "            if policy[i,j] == 'd':\n",
    "                row.append(\"down\")\n",
    "            if policy[i,j] == 'r':\n",
    "                row.append(\"right\")\n",
    "            if policy[i,j] == 'l':\n",
    "                row.append(\"left\")\n",
    "        politica.append(row)\n",
    "    return politica\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#function that debugs the position array and returns only the desired coordinates\n",
    "def coordenates(route, route_img, x_len, y_len):  \n",
    "    im = Image.open(route_img, 'r')    \n",
    "    columns = im.size[0]\n",
    "    rows = im.size[1]\n",
    "    x_center = im.size[1] /2\n",
    "    y_center = im.size[0] / 2\n",
    "    x_scale = x_len  / im.size[1] \n",
    "    y_scale = y_len  / im.size[0]\n",
    "    final_coordenates=[]   \n",
    "    for i in range (1,len (route) -1):      \n",
    "        if route [i][0] == route [i-1][0] :\n",
    "            if route [i][0] != route [i+1][0] :\n",
    "                new_coord = []\n",
    "                new_coord.append ( (route [i][0] - x_center) * x_scale )\n",
    "                new_coord.append (( route [i][1] - y_center) * y_scale )\n",
    "                final_coordenates.append (new_coord)  \n",
    "                \n",
    "        elif route [i][1] == route [i-1][1] :\n",
    "            if route [i][1] != route [i+1][1] :\n",
    "                new_coord = []\n",
    "                new_coord.append ( (route [i][0] - x_center) * x_scale )\n",
    "                new_coord.append (( route [i][1] - y_center) * y_scale )\n",
    "                final_coordenates.append (new_coord)  \n",
    "        elif route [i][0]-1 == route [i-1][0] and route [i][1]-1 == route [i-1][1]:  \n",
    "            if route [i][0]+1 != route [i+1][0] or  route [i][1]+1 != route [i+1][1]:\n",
    "                new_coord = []\n",
    "                new_coord.append ( (route [i][0] - x_center) * x_scale )\n",
    "                new_coord.append (( route [i][1] - y_center) * y_scale )\n",
    "                final_coordenates.append (new_coord)              \n",
    "            \n",
    "                \n",
    "    final_coordenates.append ([( (route [len (route)-1][0] - x_center) * x_scale ) , (( route [len (route)-1][1] - y_center) * y_scale )])   \n",
    "    \n",
    "    return final_coordenates\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        subscriber_ROS()\n",
    "        environment = GridEnvironment(x_fin,y_fin, ruta_imagen)\n",
    "        size = environment.value_state_table.shape\n",
    "        policy = run_policy(size)\n",
    "        agent = valueBasedAgent(environment, np.array(policy), discount_factor)\n",
    "        # Initialize policy evaluation class.\n",
    "        policy_evaluation = PolicyEvaluation(x_fin,y_fin,environment, agent, iterations = 15)\n",
    "        policy_evaluation.evaluate(plot_grid = False)\n",
    "        policy_evaluation.updatePolicy()\n",
    "        # Run and train the agent\n",
    "        agent.pos = [x_ini,y_ini] #13, 3\n",
    "        game = Game(x_fin,y_fin, environment, agent)\n",
    "        array_pos = game.update(0.3)\n",
    "        final_coordenates = coordenates(array_pos, ruta_imagen, x_len, y_len)\n",
    "        environment.dibujo_ruta(array_pos)\n",
    "        publisher_ROS(final_coordenates)\n",
    "    except rospy.ROSInterrupyException:\n",
    "        rospy.loginfo(\"node terminated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 80)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.value_state_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "environment.walls_and_paths[12,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion # 0\n",
      "Iteracion # 1\n",
      "Iteracion # 2\n",
      "Iteracion # 3\n",
      "Iteracion # 4\n",
      "Iteracion # 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2ebe6465dbc2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Initialize policy evaluation class.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPolicyEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_fin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_fin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdatePolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2b92fcf9b777>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, plot_grid)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDP_policy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mDP_policy_evaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2b92fcf9b777>\u001b[0m in \u001b[0;36mDP_policy_evaluation\u001b[1;34m(self, iterations, plot_grid)\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_state_table\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvironment\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalls_and_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcanChangeStateValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                         \u001b[1;31m# Set agent position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-2b92fcf9b777>\u001b[0m in \u001b[0;36mcanChangeStateValue\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m                     \u001b[0mcoords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcoords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize policy evaluation class.\n",
    "policy_evaluation = PolicyEvaluation(x_fin,y_fin,environment, agent, iterations = 15)\n",
    "policy_evaluation.evaluate(plot_grid = False)\n",
    "\n",
    "policy_evaluation.updatePolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['d', 'd', 'd', ..., 'd', 'd', 'd'],\n",
       "       ['l', 'd', 'l', ..., 'l', 'l', 'l'],\n",
       "       ['l', 'd', 'd', ..., 'l', 'l', 'l'],\n",
       "       ...,\n",
       "       ['d', 'd', 'd', ..., 'd', 'd', 'd'],\n",
       "       ['d', 'd', 'd', ..., 'd', 'd', 'd'],\n",
       "       ['d', 'd', 'd', ..., 'd', 'd', 'd']], dtype='<U1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# New agent policy after policy evaluation.\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "politica = list()\n",
    "for i in range(len(agent.policy)):\n",
    "    row = list()\n",
    "    for j in range(len(agent.policy[0])):\n",
    "        if agent.policy[i,j] == 'u':\n",
    "            row.append(\"up\")\n",
    "        if agent.policy[i,j] == 'd':\n",
    "            row.append(\"down\")\n",
    "        if agent.policy[i,j] == 'r':\n",
    "            row.append(\"right\")\n",
    "        if agent.policy[i,j] == 'l':\n",
    "            row.append(\"left\")\n",
    "    politica.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['down', 'down', 'down', ..., 'down', 'down', 'down'],\n",
       "       ['left', 'down', 'left', ..., 'left', 'left', 'left'],\n",
       "       ['left', 'down', 'down', ..., 'left', 'left', 'left'],\n",
       "       ...,\n",
       "       ['down', 'down', 'down', ..., 'down', 'down', 'down'],\n",
       "       ['down', 'down', 'down', ..., 'down', 'down', 'down'],\n",
       "       ['down', 'down', 'down', ..., 'down', 'down', 'down']], dtype='<U5')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica =np.array(politica)\n",
    "agent.policy = politica\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#environment.value_state_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Win the Game with the previous policy evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GridWorld Initialize!\n",
      "[14, 5]\n",
      "[14, 4]\n",
      "[14, 3]\n",
      "[13, 3]\n"
     ]
    }
   ],
   "source": [
    "agent.pos = [14,6] #13, 3\n",
    "array_pos.append (agent.pos)\n",
    "game = Game(x_fin,y_fin, environment, agent)\n",
    "game.update(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (array_pos)\n",
    "#agent.pos = [4, 1]\n",
    "#game = Game(x_fin,y_fin,environment, agent)\n",
    "#game.update(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#agent.pos = [0, 0]\n",
    "#game = Game(x_fin,y_fin,environment, agent)\n",
    "#game.update(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['up', 'right', 'down', 'down', 'down'],\n",
       " ['r', 'right', 'r', 'r', 'left'],\n",
       " ['up', 'up', 'up', 'up', 'r'],\n",
       " ['left', 'r', 'up', 'left', 'down'],\n",
       " ['left', 'down', 'r', 'left', 'down']]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 0], [2, 0], [2, 3]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#function that debugs the position array and returns only the desired coordinates\n",
    "def coordenates (route:list):\n",
    "    final_coordenates=[]\n",
    "    \n",
    "    for i in range (1,len (route) -1):\n",
    "        if route [i][0] == route [i-1][0] :\n",
    "            if route [i][0] != route [i+1][0] :\n",
    "                final_coordenates.append (route[i])\n",
    "        elif route [i][1] == route [i-1][1] :\n",
    "            if route [i][1] != route [i+1][1] :\n",
    "                final_coordenates.append (route[i])                      \n",
    "    final_coordenates.append (route[len (route)-1])        \n",
    "    return final_coordenates\n",
    "coordenates (array_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the array is reinitialized\n",
    "array_pos=[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
